{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550e8c5f",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "Hybrid Garbage Classification Model \n",
    "\n",
    "This script implements a hybrid model for garbage classification that combines:\n",
    "- Image features using ResNet18\n",
    "- Text features using DistilBERT\n",
    "\n",
    "The model fuses features from both modalities to improve classification performance\n",
    "beyond what either modality could achieve individually. The implementation includes\n",
    "both an initial training phase and an automatic fine-tuning phase.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bca60d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d151ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - Uses GPU if available, otherwise falls back to CPU\n",
    "# This is important for deep learning models which benefit significantly from GPU acceleration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Hyperparameters\n",
    "NUM_CLASSES = 4 # Number of garbage categories to classify\n",
    "MAX_TEXT_LENGTH = 24 # Maximum length of text tokens for BERT processing\n",
    "BATCH_SIZE = 32 # Number of samples processed in each training batch\n",
    "EPOCHS = 5 # Number of complete passes through the training dataset\n",
    "IMAGE_SIZE = 224 # Input image size (224x224 is standard for many CNN architectures)\n",
    "LEARNING_RATE = 2e-5 # Base learning rate for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960e6f8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dataset Paths - Points to the directories containing the garbage classification dataset\n",
    "# Dataset is organized in a class-based folder structure\n",
    "TRAIN_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\"\n",
    "VAL_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Val\"\n",
    "TEST_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41579acc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def extract_text_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text information from the image filename to use as textual input.\n",
    "    This function extracts the base filename, removes underscores and numbers,\n",
    "    which helps in creating clean text descriptions from filenames.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text derived from the filename\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name_no_ext, _ = os.path.splitext(file_name)\n",
    "    text = file_name_no_ext.replace('_', ' ')\n",
    "    return re.sub(r'\\d+', '', text)  # Remove numbers to clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8994db",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load dataset paths\n",
    "def get_files_with_labels(root_path):\n",
    "    \"\"\"\n",
    "    Loads all image files from a directory structure where each subdirectory\n",
    "    represents a class. Maps each class name to a numeric label.\n",
    "    \n",
    "    Args:\n",
    "        root_path: Path to the root directory containing class subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (image_paths, texts, labels) for all images in the dataset\n",
    "    \"\"\"\n",
    "    image_paths, texts, labels = [], [], []\n",
    "    class_folders = sorted(os.listdir(root_path))\n",
    "    # Create a mapping from class folder names to numeric labels\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "     # Traverse the directory structure\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(root_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for file_name in os.listdir(class_path):\n",
    "                file_path = os.path.join(class_path, file_name)\n",
    "                # Only process image files\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(file_path)\n",
    "                    texts.append(extract_text_from_path(file_path))\n",
    "                    labels.append(label_map[class_name])\n",
    "    return image_paths, texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd312ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Pipeline for Training Images\n",
    "# These transformations increase the diversity of training data and help prevent overfitting\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), # Resize to standard input size\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(20), # Randomly rotate images by up to 20 degrees\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1), # Randomly adjust brightness, contrast, saturation, hue\n",
    "    transforms.ToTensor(), # Convert image to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalize with ImageNet means and stds\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8053d11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Transformation Pipeline for Validation/Test Images\n",
    "# No augmentation for evaluation, just preprocessing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7c93c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that handles both image and text modalities.\n",
    "    Loads images from disk and processes text through a BERT tokenizer.\n",
    "    \n",
    "    This enables the model to work with both types of input simultaneously.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, texts, labels, tokenizer, transform, max_len=MAX_TEXT_LENGTH):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with paths, texts, and preprocessing components.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of paths to image files\n",
    "            texts: List of text descriptions corresponding to each image\n",
    "            labels: List of class labels for each image\n",
    "            tokenizer: BERT tokenizer for text processing\n",
    "            transform: Image transformation pipeline\n",
    "            max_len: Maximum token length for text inputs\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset, including both image and text.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing processed image tensor, tokenized text, and label\n",
    "        \"\"\"\n",
    "        # Load and process image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Process text with BERT tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding='max_length', truncation=True,\n",
    "            max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "# Return all components needed for model input\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c398a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Attention-based Fusion Module\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism for multimodal fusion, which learns to weight\n",
    "    the importance of each modality (image vs text) for classification.\n",
    "    \n",
    "    This adaptive weighting is more sophisticated than simple concatenation\n",
    "    or averaging, as it can emphasize the more informative modality for each sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=1024, text_dim=1024):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(img_dim + text_dim, 1)\n",
    "\n",
    "    def forward(self, img_features, text_features):\n",
    "        \"\"\"\n",
    "        Compute weighted combination of image and text features.\n",
    "        \n",
    "        Args:\n",
    "            img_features: Image feature tensor\n",
    "            text_features: Text feature tensor\n",
    "            \n",
    "        Returns:\n",
    "            Fused feature tensor with adaptive weighting\n",
    "        \"\"\"\n",
    "         # Compute attention weight (importance of image vs text)\n",
    "        weights = torch.sigmoid(self.attn(torch.cat([img_features, text_features], dim=1)))\n",
    "        # Weighted combination of features\n",
    "        return weights * img_features + (1 - weights) * text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808f6ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end multimodal model that processes both images and text,\n",
    "    and combines their features for classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Image Branch: ResNet50 → FC → Normalization → ReLU\n",
    "    2. Text Branch: DistilBERT → FC → Normalization → ReLU\n",
    "    3. Attention Fusion: Combines features from both branches\n",
    "    4. Classifier: Makes final class prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        \"\"\"\n",
    "        Initialize the complete model architecture.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Image Processing Branch\n",
    "        # Load pretrained ResNet50 and remove the classification head\n",
    "        self.image_model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "        self.image_model = nn.Sequential(*list(self.image_model.children())[:-1])\n",
    "        # Feature transformation network for image embeddings\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),  # Reduce dimensionality from 2048 to 1024\n",
    "            nn.BatchNorm1d(1024), # Normalize activations for stability\n",
    "            nn.ReLU()  # Apply non-linearity\n",
    "        )\n",
    "# Load pretrained DistilBERT for text feature extraction\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # Feature transformation network for text embeddings\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(768, 1024), # Transform from BERT's 768 dimension to 1024\n",
    "            nn.BatchNorm1d(1024),  # Normalize activations\n",
    "            nn.ReLU() # Apply non-linearity\n",
    "        )\n",
    "# Attention-based fusion module to combine modalities\n",
    "        self.fusion = AttentionFusion(img_dim=1024, text_dim=1024)\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "        # Process images through ResNet\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire model.\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of input images\n",
    "            input_ids: Tokenized text inputs\n",
    "            attention_mask: Attention mask for BERT\n",
    "            \n",
    "        Returns:\n",
    "            Classification logits\n",
    "        \"\"\"\n",
    "        img_features = self.image_model(images).squeeze()\n",
    "        # Ensure img_features has the right shape\n",
    "        if len(img_features.shape) == 1:\n",
    "            img_features = img_features.unsqueeze(0)\n",
    "        img_features = self.image_fc(img_features)\n",
    "        \n",
    "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        text_features = self.text_fc(text_output[:, 0, :])\n",
    "        fused_features = self.fusion(img_features, text_features)\n",
    "        return self.classifier(fused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e7cc5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Trains the model with periodic validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        scheduler: Learning rate scheduler\n",
    "        epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        Trained model (with best weights loaded)\n",
    "      \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, input_ids, attn_mask, labels = batch['image'].to(device), batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, input_ids, attn_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct_train += (predictions == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_acc = correct_train / total_train\n",
    "        train_loss /= len(train_loader)\n",
    "# Validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = 0, 0, 0\n",
    " # No gradient computation needed for validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                images, input_ids, attn_mask, labels = batch['image'].to(device), batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "                outputs = model(images, input_ids, attn_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct_val += (predictions == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_acc = correct_val / total_val\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    " # Update learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_hybrid_model.pth')\n",
    "            print(\"Saved Best Model!\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_hybrid_model.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b20085",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, input_ids, attn_mask, labels = batch['image'].to(device), batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "            outputs = model(images, input_ids, attn_mask)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f70789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Load datasets\n",
    "    train_image_paths, train_texts, train_labels = get_files_with_labels(TRAIN_PATH)\n",
    "    val_image_paths, val_texts, val_labels = get_files_with_labels(VAL_PATH)\n",
    "    test_image_paths, test_texts, test_labels = get_files_with_labels(TEST_PATH)\n",
    "\n",
    "    train_dataset = HybridDataset(train_image_paths, train_texts, train_labels, tokenizer, train_transform)\n",
    "    val_dataset = HybridDataset(val_image_paths, val_texts, val_labels, tokenizer, test_transform)\n",
    "    test_dataset = HybridDataset(test_image_paths, test_texts, test_labels, tokenizer, test_transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize model\n",
    "    model = HybridModel().to(device)\n",
    "\n",
    "    # Define optimizer and scheduler\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": model.image_model.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.text_model.parameters(), \"lr\": 5e-6},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 1e-4},\n",
    "    ])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=EPOCHS)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

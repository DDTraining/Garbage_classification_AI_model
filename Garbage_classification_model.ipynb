{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
},
   "source": [
    "# Hybrid Garbage Classification Model\n",
    "\n",
    "This notebook implements a hybrid model for garbage classification that combines:\n",
    "- Image features using ResNet50 (pretrained on ImageNet)\n",
    "- Text features using DistilBERT (pretrained on general text corpus)\n",
    "\n",
    "The model fuses features from both modalities using an attention mechanism to improve classification performance beyond what either modality could achieve individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device\n",
    "We'll check if a GPU is available and use it for training, which can significantly speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - Uses GPU if available, otherwise falls back to CPU\n",
    "# This is important for deep learning models which benefit significantly from GPU acceleration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Define constants and configuration parameters for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Hyperparameters\n",
    "NUM_CLASSES = 4       # Number of garbage categories to classify\n",
    "MAX_TEXT_LENGTH = 24  # Maximum length of text tokens for BERT processing\n",
    "BATCH_SIZE = 32       # Number of samples processed in each training batch\n",
    "EPOCHS = 5            # Number of complete passes through the training dataset\n",
    "IMAGE_SIZE = 224      # Input image size (224x224 is standard for many CNN architectures)\n",
    "LEARNING_RATE = 2e-5  # Base learning rate for optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Paths\n",
    "Specify the locations of training, validation, and test datasets. The datasets are organized in class-specific directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths - Points to the directories containing the garbage classification dataset\n",
    "# Dataset is organized in a class-based folder structure\n",
    "TRAIN_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\"\n",
    "VAL_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Val\"\n",
    "TEST_PATH = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction and Preprocessing\n",
    "Define a function to extract meaningful text from image filenames to use as textual input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Function\n",
    "def extract_text_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text information from the image filename to use as textual input.\n",
    "    This function extracts the base filename, removes underscores and numbers,\n",
    "    which helps in creating clean text descriptions from filenames.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text derived from the filename\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name_no_ext, _ = os.path.splitext(file_name)\n",
    "    text = file_name_no_ext.replace('_', ' ')\n",
    "    return re.sub(r'\\d+', '', text)  # Remove numbers to clean up text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "Define a function to load image paths, extract text descriptions, and assign numeric labels based on directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading Function\n",
    "def get_files_with_labels(root_path):\n",
    "    \"\"\"\n",
    "    Loads all image files from a directory structure where each subdirectory\n",
    "    represents a class. Maps each class name to a numeric label.\n",
    "    \n",
    "    Args:\n",
    "        root_path: Path to the root directory containing class subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (image_paths, texts, labels) for all images in the dataset\n",
    "    \"\"\"\n",
    "    image_paths, texts, labels = [], [], []\n",
    "    class_folders = sorted(os.listdir(root_path))\n",
    "    # Create a mapping from class folder names to numeric labels\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "\n",
    "    # Traverse the directory structure\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(root_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for file_name in os.listdir(class_path):\n",
    "                file_path = os.path.join(class_path, file_name)\n",
    "                # Only process image files\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(file_path)\n",
    "                    texts.append(extract_text_from_path(file_path))\n",
    "                    labels.append(label_map[class_name])\n",
    "    return image_paths, texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transformations\n",
    "Define augmentation pipelines for training and evaluation datasets. Training includes random transformations to improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Pipeline for Training Images\n",
    "# These transformations increase the diversity of training data and help prevent overfitting\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize to standard input size\n",
    "    transforms.RandomHorizontalFlip(),            # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(20),                # Randomly rotate images by up to 20 degrees\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),   # Randomly adjust brightness, contrast, saturation, hue\n",
    "    transforms.ToTensor(),                        # Convert image to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],   # Normalize with ImageNet means and stds\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Pipeline for Validation/Test Images\n",
    "# No augmentation for evaluation, just preprocessing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize to standard input size\n",
    "    transforms.ToTensor(),                        # Convert image to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],   # Normalize with ImageNet means and stds\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "Define a dataset class that handles both image and text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Multimodal Input\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class that handles both image and text modalities.\n",
    "    Loads images from disk and processes text through a BERT tokenizer.\n",
    "    \n",
    "    This enables the model to work with both types of input simultaneously.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, texts, labels, tokenizer, transform, max_len=MAX_TEXT_LENGTH):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with paths, texts, and preprocessing components.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of paths to image files\n",
    "            texts: List of text descriptions corresponding to each image\n",
    "            labels: List of class labels for each image\n",
    "            tokenizer: BERT tokenizer for text processing\n",
    "            transform: Image transformation pipeline\n",
    "            max_len: Maximum token length for text inputs\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset, including both image and text.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing processed image tensor, tokenized text, and label\n",
    "        \"\"\"\n",
    "        # Load and process image\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Process text with BERT tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx], padding='max_length', truncation=True,\n",
    "            max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Return all components needed for model input\n",
    "        return {\n",
    "            'image': image,                                  # Processed image tensor\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),   # Tokenized text IDs\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # Attention mask for padding\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)  # Class label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Fusion Module\n",
    "Define a mechanism to adaptively combine features from both modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention-based Fusion Module\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism for multimodal fusion, which learns to weight\n",
    "    the importance of each modality (image vs text) for classification.\n",
    "    \n",
    "    This adaptive weighting is more sophisticated than simple concatenation\n",
    "    or averaging, as it can emphasize the more informative modality for each sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=1024, text_dim=1024):\n",
    "        \"\"\"\n",
    "        Initialize the fusion module with dimensions of both feature types.\n",
    "        \n",
    "        Args:\n",
    "            img_dim: Dimension of image features\n",
    "            text_dim: Dimension of text features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Linear layer to compute attention weights from concatenated features\n",
    "        self.attn = nn.Linear(img_dim + text_dim, 1)\n",
    "\n",
    "    def forward(self, img_features, text_features):\n",
    "        \"\"\"\n",
    "        Compute weighted combination of image and text features.\n",
    "        \n",
    "        Args:\n",
    "            img_features: Image feature tensor\n",
    "            text_features: Text feature tensor\n",
    "            \n",
    "        Returns:\n",
    "            Fused feature tensor with adaptive weighting\n",
    "        \"\"\"\n",
    "        # Compute attention weight (importance of image vs text)\n",
    "        weights = torch.sigmoid(self.attn(torch.cat([img_features, text_features], dim=1)))\n",
    "        # Weighted combination of features\n",
    "        return weights * img_features + (1 - weights) * text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model Definition\n",
    "Define the complete model architecture that combines image and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Hybrid Model Architecture\n",
    "class HybridModel(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end multimodal model that processes both images and text,\n",
    "    and combines their features for classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Image Branch: ResNet50 → FC → Normalization → ReLU\n",
    "    2. Text Branch: DistilBERT → FC → Normalization → ReLU\n",
    "    3. Attention Fusion: Combines features from both branches\n",
    "    4. Classifier: Makes final class prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        \"\"\"\n",
    "        Initialize the complete model architecture.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Image Processing Branch\n",
    "        # Load pretrained ResNet50 and remove the classification head\n",
    "        self.image_model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "        self.image_model = nn.Sequential(*list(self.image_model.children())[:-1])\n",
    "        # Feature transformation network for image embeddings\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),         # Reduce dimensionality from 2048 to 1024\n",
    "            nn.BatchNorm1d(1024),          # Normalize activations for stability\n",
    "            nn.ReLU()                      # Apply non-linearity\n",
    "        )\n",
    "\n",
    "        # Text Processing Branch\n",
    "        # Load pretrained DistilBERT for text feature extraction\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # Feature transformation network for text embeddings\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(768, 1024),          # Transform from BERT's 768 dimension to 1024\n",
    "            nn.BatchNorm1d(1024),          # Normalize activations\n",
    "            nn.ReLU()                      # Apply non-linearity\n",
    "        )\n",
    "\n",
    "        # Attention-based fusion module to combine modalities\n",
    "        self.fusion = AttentionFusion(img_dim=1024, text_dim=1024)\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire model.\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of input images\n",
    "            input_ids: Tokenized text inputs\n",
    "            attention_mask: Attention mask for BERT\n",
    "            \n",
    "        Returns:\n",
    "            Classification logits\n",
    "        \"\"\"\n",
    "        # Process images through ResNet\n",
    "        img_features = self.image_model(images).squeeze()\n",
    "        # Ensure img_features has the right shape (handle batch size of 1)\n",
    "        if len(img_features.shape) == 1:\n",
    "            img_features = img_features.unsqueeze(0)\n",
    "        # Transform image features to common 1024D space\n",
    "        img_features = self.image_fc(img_features)\n",
    "        \n",
    "        # Process text through DistilBERT\n",
    "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # Use CLS token output as text representation\n",
    "        text_features = self.text_fc(text_output[:, 0, :])\n",
    "        \n",
    "        # Combine features using attention fusion\n",
    "        fused_features = self.fusion(img_features, text_features)\n",
    "        # Get final classification logits\n",
    "        return self.classifier(fused_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "Define function to train the model with validation and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function with Validation\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Trains the model with periodic validation and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        scheduler: Learning rate scheduler\n",
    "        epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        Trained model (with best weights loaded)\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')  # Track best validation loss for model selection\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0, 0, 0\n",
    "\n",
    "        # Iterate through training batches with progress bar\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            # Move data to device (GPU/CPU)\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward and backward pass\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(images, input_ids, attn_mask)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            predictions = outputs.argmax(dim=1)  # Get predicted classes\n",
    "            correct_train += (predictions == labels).sum().item()  # Count correct predictions\n",
    "            total_train += labels.size(0)  # Total number of samples\n",
    "\n",
    "        # Calculate epoch-level metrics\n",
    "        train_acc = correct_train / total_train\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, correct_val, total_val = 0, 0, 0\n",
    "\n",
    "        # No gradient computation needed for validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                # Move data to device\n",
    "                images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attn_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass only\n",
    "                outputs = model(images, input_ids, attn_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Track validation metrics\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct_val += (predictions == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_acc = correct_val / total_val\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Update learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Model checkpointing - save best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_hybrid_model.pth')\n",
    "            print(\"Saved Best Model!\")\n",
    "\n",
    "    # Load best model weights for final return\n",
    "    model.load_state_dict(torch.load('best_hybrid_model.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "Define function to evaluate model performance on test data and generate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function for Test Set\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a test set and generates performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: DataLoader for test data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds, all_labels = [], []  # Track all predictions and true labels\n",
    "\n",
    "    # No gradient computation needed for testing\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Move data to device\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attn_mask)\n",
    "            preds = outputs.argmax(dim=1)  # Get predicted classes\n",
    "\n",
    "            # Collect results (convert to CPU and numpy for metrics calculation)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Generate detailed classification metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    # Create and display confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Initialize, train, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize tokenizer for text processing\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
